---
title: "Comparing the Four Approaches: Flight Price Prediction"
format:
  html:
    toc: true
    toc-depth: 4
    html-math-method: katex
    fontsize: 11pt
    fig-width: 8
    fig-height: 6
    df-print: paged
    code-overflow: wrap
    self-contained: true
editor: visual
---

.scrolling { max-height: 300px; overflow-y: auto; max-width: 600px; }

# Data Preparation

```{r setup}
#| label: setup
#| warning: false
#| message: false 

# Load required libraries
library(tidyverse)
library(caTools)
library(rpart)
library(rpart.plot)
library(caret)
library(Boruta)
library(cvms)
library(dplyr)
library(caret)
library(kableExtra)
library(gridExtra)
library(glmnet)
library(gbm)
```

```{r}
#| warning: false
#| message: false 

# Load training data
flight_data_train <- read_csv("~/FlightProject/flight_data_train.csv") %>%
  as.data.frame() %>%
  select(-data_set_division)

# Load test data  
flight_data_test <- read_csv("~/FlightProject/flight_data_test.csv") %>%
  as.data.frame()%>%
  select(-data_set_division)

# Display training data structure
glimpse(flight_data_train)
```

Replacing the missing values for airport stops with NONE.

```{r data-cleaning}
#| label: data-cleaning
#| warning: false
#| message: false 

flight_data_train <- flight_data_train %>%
  mutate(
    # Replace missing airport stops with "NONE" 
    airport_code_first_stop = ifelse(is.na(airport_code_first_stop), "NONE", airport_code_first_stop),
    airport_code_second_stop = ifelse(is.na(airport_code_second_stop), "NONE", airport_code_second_stop),
    airport_code_third_stop = ifelse(is.na(airport_code_third_stop), "NONE", airport_code_third_stop)
  )

flight_data_test <- flight_data_test %>%
  mutate(
    # Replace missing airport stops with "NONE" 
    airport_code_first_stop = ifelse(is.na(airport_code_first_stop), "NONE", airport_code_first_stop),
    airport_code_second_stop = ifelse(is.na(airport_code_second_stop), "NONE", airport_code_second_stop),
    airport_code_third_stop = ifelse(is.na(airport_code_third_stop), "NONE", airport_code_third_stop)
  )
```

Feature Engineering

```{r feature-engineering}
#| label: feature-engineering
#| warning: false
#| message: false 

# Create engineered features for training data
flight_data_train_processed <- flight_data_train %>%
  mutate(
    # Convert categorical stops to numeric feature
    number_of_stops = case_when(
      Total_Stops == "non-stop" ~ 0,
      Total_Stops == "1 stop" ~ 1,
      Total_Stops == "2 stops" ~ 2,
      Total_Stops == "3 stops" ~ 3,
      Total_Stops == "4 stops" ~ 4
    ),
    # Create weekend indicator feature
    weekend = case_when(
      departure_day_of_week %in% c("Sunday", "Saturday") ~ "Weekend",
      TRUE ~ "Weekday"
    )
  )

# Apply same transformations to test data
flight_data_test_processed <- flight_data_test %>%
  mutate(
    number_of_stops = case_when(
      Total_Stops == "non-stop" ~ 0,
      Total_Stops == "1 stop" ~ 1,
      Total_Stops == "2 stops" ~ 2,
      Total_Stops == "3 stops" ~ 3,
      Total_Stops == "4 stops" ~ 4
    ),
    weekend = case_when(
      departure_day_of_week %in% c("Sunday", "Saturday") ~ "Weekend",
      TRUE ~ "Weekday"
    )
  )
```

```{r}
#| warning: false
#| message: false 

# Create numeric versions for algorithms that need them
# Training data numeric conversions
airport_code_bind <- flight_data_train_processed %>%
  mutate(id = row_number()) %>%
  select(id, airport_code_arrival, airport_code_departure, 
          airport_code_first_stop, airport_code_second_stop, 
          airport_code_third_stop) %>%
  pivot_longer(!id, names_to = "item", values_to = "response") %>%
  mutate(item = paste0(item, "_numeric"),
         response_numeric = as.numeric(as.factor(response))) %>%
  pivot_wider(id_cols = id, names_from = item, values_from = response_numeric)

flight_data_train_processed <- cbind(flight_data_train_processed, airport_code_bind)

flight_data_train_processed$Airline_numeric <- as.numeric(factor(flight_data_train_processed$Airline))
flight_data_train_processed$weekend_numeric <- as.numeric(factor(flight_data_train_processed$weekend))
flight_data_train_processed$departure_date_time_numeric <- as.numeric(flight_data_train_processed$departure_date_time)
flight_data_train_processed$arrival_date_time_numeric <- as.numeric(flight_data_train_processed$arrival_date_time)
flight_data_train_processed$Date_of_Journey_numeric <- as.numeric(as.factor(as.character(flight_data_train_processed$Date_of_Journey)))

# Test data numeric conversions
airport_code_bind <- flight_data_test_processed %>%
  mutate(id = row_number()) %>%
  select(id, airport_code_arrival, airport_code_departure, 
          airport_code_first_stop, airport_code_second_stop, 
          airport_code_third_stop) %>%
  pivot_longer(!id, names_to = "item", values_to = "response") %>%
  mutate(item = paste0(item, "_numeric"),
         response_numeric = as.numeric(as.factor(response))) %>%
  pivot_wider(id_cols = id, names_from = item, values_from = response_numeric)

flight_data_test_processed <- cbind(flight_data_test_processed, airport_code_bind)

flight_data_test_processed$Airline_numeric <- as.numeric(factor(flight_data_test_processed$Airline))
flight_data_test_processed$weekend_numeric <- as.numeric(factor(flight_data_test_processed$weekend))
flight_data_test_processed$departure_date_time_numeric <- as.numeric(flight_data_test_processed$departure_date_time)
flight_data_test_processed$arrival_date_time_numeric <- as.numeric(flight_data_test_processed$arrival_date_time)
flight_data_test_processed$Date_of_Journey_numeric <- as.numeric(as.factor(as.character(flight_data_test_processed$Date_of_Journey)))
```

```{r}
#| warning: false
#| message: false 

# Select final columns for modeling
flight_data_train_processed <- flight_data_train_processed %>%
  select(Departure_Day, Departure_Month, Departure_Hour, Duration_Hours,
         number_of_stops, weekend, Date_of_Journey, departure_date_time,
         arrival_date_time, Airline, airport_code_arrival,
         airport_code_departure, airport_code_first_stop,
         airport_code_second_stop, airport_code_third_stop, Price,
         Airline_numeric, weekend_numeric, arrival_date_time_numeric, Date_of_Journey_numeric,
         airport_code_arrival_numeric, airport_code_departure_numeric, airport_code_first_stop_numeric,
         airport_code_second_stop_numeric, airport_code_third_stop_numeric, departure_date_time_numeric)

flight_data_test_processed <- flight_data_test_processed %>%
  select(Departure_Day, Departure_Month, Departure_Hour, Duration_Hours,
         number_of_stops, weekend, Date_of_Journey, departure_date_time,
         arrival_date_time, Airline, airport_code_arrival,
         airport_code_departure, airport_code_first_stop,
         airport_code_second_stop, airport_code_third_stop, Price,
         Airline_numeric, weekend_numeric, arrival_date_time_numeric, Date_of_Journey_numeric,
         airport_code_arrival_numeric, airport_code_departure_numeric, airport_code_first_stop_numeric,
         airport_code_second_stop_numeric, airport_code_third_stop_numeric, departure_date_time_numeric)
```

# Model 1: Linear Regression

## Build Model

```{r}
#| warning: false
#| message: false 

regression.flight <- lm(Price ~ Departure_Day + Departure_Month + Departure_Hour + Duration_Hours + 
                          number_of_stops + weekend + Date_of_Journey + 
                          departure_date_time + arrival_date_time + 
                          airport_code_first_stop + airport_code_second_stop + airport_code_third_stop +
                          Airline + airport_code_departure + airport_code_arrival,
                          data = flight_data_train_processed)
```

## Evaluate Linear Regression

```{r}
#| warning: false
#| message: false 

# Make predictions on test set
linear_predictions <- predict(regression.flight, newdata = flight_data_test_processed)

# Calculate metrics
rmse_linear <- sqrt(mean((flight_data_test_processed$Price - linear_predictions)^2))
r_squared_linear <- cor(flight_data_test_processed$Price, linear_predictions)^2

# Cross-validation
set.seed(42)
train_control <- trainControl(method = "cv", number = 5)
cv_linear <- train(Price ~ Departure_Day + Departure_Month + Departure_Hour + Duration_Hours + 
                   number_of_stops + weekend + Date_of_Journey + 
                   departure_date_time + arrival_date_time + 
                   airport_code_first_stop + airport_code_second_stop + airport_code_third_stop +
                   Airline + airport_code_departure + airport_code_arrival,
                   data = flight_data_train_processed,
                   method = "lm",
                   trControl = train_control)

cv_rmse_linear <- cv_linear$results$RMSE

cat("Linear Regression Performance:\n")
cat("Test RMSE:", round(rmse_linear, 2), "\n")
cat("Test R-squared:", round(r_squared_linear, 3), "\n")
cat("CV RMSE:", round(cv_rmse_linear, 2), "\n")
```

# Model 2: LASSO Regression

## Build LASSO Model

```{r}
#| warning: false
#| message: false 

# Combine train and test to ensure consistent factor levels
combined_data <- rbind(
  flight_data_train_processed %>% mutate(dataset = "train"),
  flight_data_test_processed %>% mutate(dataset = "test")
)

# Create model matrix for combined data to ensure consistent variables
X_combined <- model.matrix(Price ~ Departure_Day + Departure_Month + Departure_Hour + Duration_Hours + 
                          number_of_stops + weekend + Date_of_Journey + 
                          departure_date_time + arrival_date_time + 
                          airport_code_first_stop + airport_code_second_stop + airport_code_third_stop +
                          Airline + airport_code_departure + airport_code_arrival - 1, 
                  data = combined_data)

# Split back into train and test
train_indices <- which(combined_data$dataset == "train")
test_indices <- which(combined_data$dataset == "test")

X <- X_combined[train_indices, ]
X_test <- X_combined[test_indices, ]

y <- flight_data_train_processed$Price

# Fit LASSO with cross-validation
set.seed(42)
cv_lasso_flight <- cv.glmnet(X, y, alpha = 1, nfolds = 10)
lambda_min <- cv_lasso_flight$lambda.min      

# Final model
final_flight_model <- glmnet(X, y, alpha = 1, lambda = lambda_min)
```

## Evaluate LASSO

```{r}
#| warning: false
#| message: false 

# Make predictions
lasso_predictions_test <- predict(final_flight_model, newx = X_test)

# Calculate metrics
rmse_lasso <- sqrt(mean((flight_data_test_processed$Price - lasso_predictions_test)^2))
r_squared_lasso <- cor(flight_data_test_processed$Price, lasso_predictions_test)^2
cv_rmse_lasso <- sqrt(cv_lasso_flight$cvm[cv_lasso_flight$lambda == lambda_min])

cat("LASSO Regression Performance:\n")
cat("Test RMSE:", round(rmse_lasso, 2), "\n")
cat("Test R-squared:", round(r_squared_lasso, 3), "\n")
cat("CV RMSE:", round(cv_rmse_lasso, 2), "\n")
```

# Model 3: Decision Trees

## Build Decision Tree

```{r}
#| warning: false
#| message: false 

set.seed(42)
model_deep <- rpart(
  Price ~ Departure_Day + Departure_Month + Departure_Hour + Duration_Hours +
    number_of_stops + weekend + Date_of_Journey + departure_date_time +
    arrival_date_time + Airline + airport_code_arrival +
    airport_code_departure + airport_code_first_stop +
    airport_code_second_stop + airport_code_third_stop,
  data = flight_data_train_processed, 
  method = "anova",
  minsplit = 8, minbucket = 2, maxdepth = 30
)
```

## Evaluate Decision Tree

```{r}
#| warning: false
#| message: false 

# Make predictions on test set
tree_predictions <- predict(model_deep, flight_data_test_processed)

# Calculate test metrics
rmse_tree <- sqrt(mean((flight_data_test_processed$Price - tree_predictions)^2))
r_squared_tree <- cor(flight_data_test_processed$Price, tree_predictions)^2

# Stratified Cross-Validation (matching your earlier analysis)
# Create price bins for stratification
flight_data_train_processed$price_bin <- cut(flight_data_train_processed$Price, 
                                           breaks = quantile(flight_data_train_processed$Price, 
                                                           probs = c(0, 0.25, 0.5, 0.75, 1.0)),
                                           labels = c("Low", "Medium", "High", "Very_High"),
                                           include.lowest = TRUE)

# Pre-process factor levels to ensure consistency
factor_cols <- c("Airline", "airport_code_arrival", "airport_code_departure", 
                 "airport_code_first_stop", "airport_code_second_stop", 
                 "airport_code_third_stop", "weekend")

for(col in factor_cols) {
  flight_data_train_processed[[col]] <- factor(flight_data_train_processed[[col]])
}

# Create stratified folds
set.seed(42)
stratified_folds <- createFolds(flight_data_train_processed$price_bin, 
                               k = 10, list = TRUE, returnTrain = FALSE)

# Stratified CV function
stratified_cv_rpart <- function(data, folds) {
  cv_results <- data.frame(fold = integer(), rmse = numeric(), 
                          r_squared = numeric(), mae = numeric())
  
  for(i in 1:length(folds)) {
    test_indices <- folds[[i]]
    train_data <- data[-test_indices, ]
    test_data <- data[test_indices, ]
    
    # Ensure consistent factor levels
    for(col in factor_cols) {
      train_data[[col]] <- factor(train_data[[col]], levels = levels(data[[col]]))
      test_data[[col]] <- factor(test_data[[col]], levels = levels(data[[col]]))
    }
    
    # Fit model
    model <- rpart(Price ~ Departure_Day + Departure_Month + Departure_Hour + Duration_Hours +
                   number_of_stops + weekend + Date_of_Journey + departure_date_time +
                   arrival_date_time + Airline + airport_code_arrival +
                   airport_code_departure + airport_code_first_stop +
                   airport_code_second_stop + airport_code_third_stop,
                   data = train_data, method = "anova",
                   minsplit = 8, minbucket = 2, maxdepth = 30)
    
    predictions <- predict(model, test_data)
    rmse <- sqrt(mean((test_data$Price - predictions)^2))
    r_squared <- cor(test_data$Price, predictions)^2
    mae <- mean(abs(test_data$Price - predictions))
    
    cv_results <- rbind(cv_results, data.frame(
      fold = i, rmse = rmse, r_squared = r_squared, mae = mae))
  }
  
  return(cv_results)
}

# Run stratified CV
tree_cv_results <- stratified_cv_rpart(flight_data_train_processed, stratified_folds)
cv_rmse_tree <- mean(tree_cv_results$rmse)

cat("Decision Tree Performance:\n")
cat("Test RMSE:", round(rmse_tree, 2), "\n")
cat("Test R-squared:", round(r_squared_tree, 3), "\n")
cat("CV RMSE:", round(cv_rmse_tree, 2), "\n")
```

# Model 4: Boosted Decision Trees (XGBoost)

## Build XGBoost Model

```{r}
#| warning: false
#| message: false 

# Load xgboost library
library(xgboost)

# Prepare data for XGBoost (needs numeric matrix)
xgb_features <- c("Departure_Day", "Departure_Month", "Departure_Hour", 
                  "Duration_Hours", "number_of_stops", "weekend_numeric", 
                  "Date_of_Journey_numeric", "departure_date_time_numeric", 
                  "arrival_date_time_numeric", "Airline_numeric", 
                  "airport_code_arrival_numeric", "airport_code_departure_numeric", 
                  "airport_code_first_stop_numeric", "airport_code_second_stop_numeric", 
                  "airport_code_third_stop_numeric")

# Create training matrix
train_matrix <- as.matrix(flight_data_train_processed[, xgb_features])
train_label <- flight_data_train_processed$Price

# Create test matrix  
test_matrix <- as.matrix(flight_data_test_processed[, xgb_features])

# Convert to xgb.DMatrix
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label)
dtest <- xgb.DMatrix(data = test_matrix, label = flight_data_test_processed$Price)

# Set XGBoost parameters
params <- list(
  objective = "reg:squarederror",
  eta = 0.1,                    # learning rate
  max_depth = 6,                # maximum depth of trees
  subsample = 0.8,              # subsample ratio
  colsample_bytree = 0.8,       # feature sampling ratio
  seed = 42
)

# Cross-validation to find optimal number of rounds
set.seed(42)
cv_result <- xgb.cv(
  params = params,
  data = dtrain,
  nfold = 5,
  nrounds = 500,
  early_stopping_rounds = 50,
  verbose = FALSE,
  print_every_n = 0
)

# Get optimal number of rounds
optimal_rounds <- cv_result$best_iteration

# Train final model
set.seed(42)
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = optimal_rounds,
  verbose = FALSE
)
```

## Evaluate XGBoost

```{r}
#| warning: false
#| message: false 

# Make predictions
xgb_predictions <- predict(xgb_model, dtest)

# Calculate metrics
rmse_boosted <- sqrt(mean((flight_data_test_processed$Price - xgb_predictions)^2))
r_squared_boosted <- cor(flight_data_test_processed$Price, xgb_predictions)^2

# Get CV error correctly - the evaluation log stores RMSE directly, not squared error
cv_rmse_boosted <- cv_result$evaluation_log$test_rmse_mean[optimal_rounds]

# Debug: let's check what's in the evaluation log
cat("Debug - First few CV RMSE values:", head(cv_result$evaluation_log$test_rmse_mean), "\n")
cat("Debug - CV RMSE at optimal round:", cv_rmse_boosted, "\n")

cat("XGBoost Performance:\n")
cat("Test RMSE:", round(rmse_boosted, 2), "\n")
cat("Test R-squared:", round(r_squared_boosted, 3), "\n")
cat("CV RMSE:", round(cv_rmse_boosted, 2), "\n")
cat("Optimal rounds:", optimal_rounds, "\n")
```

# Model Comparison Summary

```{r}
#| warning: false
#| message: false 

# Create comprehensive comparison table
model_comparison <- data.frame(
  Model = c("Linear Regression", "LASSO Regression", "Decision Tree", "Boosted Trees"),
  Test_RMSE = c(rmse_linear, rmse_lasso, rmse_tree, rmse_boosted),
  Test_R_squared = c(r_squared_linear, r_squared_lasso, r_squared_tree, r_squared_boosted),
  CV_RMSE = c(cv_rmse_linear, cv_rmse_lasso, cv_rmse_tree, cv_rmse_boosted),
  RMSE_Rank = rank(c(rmse_linear, rmse_lasso, rmse_tree, rmse_boosted)),
  R_squared_Rank = rank(-c(r_squared_linear, r_squared_lasso, r_squared_tree, r_squared_boosted))
)

# Round for display
model_comparison$Test_RMSE <- round(model_comparison$Test_RMSE, 2)
model_comparison$Test_R_squared <- round(model_comparison$Test_R_squared, 3)
model_comparison$CV_RMSE <- round(model_comparison$CV_RMSE, 2)

# Display table
model_comparison %>%
  kbl(caption = "Model Performance Comparison",
      col.names = c("Model", "Test RMSE", "Test R²", "CV RMSE", "RMSE Rank", "R² Rank")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = FALSE, position = "left") %>%
  row_spec(which.min(model_comparison$Test_RMSE), bold = TRUE, color = "white", background = "#2E8B57")
```

## Key Findings

```{r}
#| echo: false
#| warning: false
#| message: false 

best_model_rmse <- model_comparison$Model[which.min(model_comparison$Test_RMSE)]
best_model_r2 <- model_comparison$Model[which.max(model_comparison$Test_R_squared)]
worst_model_rmse <- model_comparison$Model[which.max(model_comparison$Test_RMSE)]

cat("PERFORMANCE SUMMARY:\n")
cat("==================\n")
cat("Best model by RMSE:", best_model_rmse, 
    "(", min(model_comparison$Test_RMSE), ")\n")
cat("Best model by R²:", best_model_r2, 
    "(", max(model_comparison$Test_R_squared), ")\n")
cat("Worst model by RMSE:", worst_model_rmse, 
    "(", max(model_comparison$Test_RMSE), ")\n")

# Calculate improvement
rmse_improvement <- round((max(model_comparison$Test_RMSE) - min(model_comparison$Test_RMSE)) / 
                         max(model_comparison$Test_RMSE) * 100, 1)
cat("\nRMSE improvement from worst to best:", rmse_improvement, "%\n")

# Mean price for context
mean_price <- mean(flight_data_test_processed$Price)
cat("Mean flight price: $", round(mean_price, 2), "\n")
cat("Best model error as % of mean price:", 
    round(min(model_comparison$Test_RMSE) / mean_price * 100, 1), "%\n")
```

## Business Interpretation

Based on these results, here's what each model tells us and the critical insights revealed:

### Model Performance Analysis

**Linear Regression**: Provides a solid baseline with good interpretability and **consistent performance** between cross-validation (CV RMSE: 2,140) and test set (Test RMSE: 1,961). Shows which factors have the strongest linear relationships with price.

**LASSO Regression**: Automatically selects the most important features while maintaining similar performance to linear regression. Also shows **excellent consistency** between CV and test performance. Helps identify which variables truly matter for flight pricing.

**Decision Trees**: Offers excellent interpretability with rules that business stakeholders can understand. Shows reasonable consistency between CV (2,443) and test (2,301) performance, though overall accuracy is lower.

**XGBoost (Boosted Trees)**: Reveals a **critical model reliability issue**. Despite excellent cross-validation performance (CV RMSE: 1,851), it performs poorly on the test set (Test RMSE: 3,738). This large gap indicates the model is **overfitting** and would likely fail in production.

### Key Data Science Insight: The Generalization Gap

The XGBoost results demonstrate a fundamental machine learning principle: **cross-validation performance doesn't guarantee real-world performance**. The 2,000-point difference between CV and test RMSE suggests:

-   The model memorized patterns specific to the training data
-   The training and test sets may have different underlying distributions
-   Complex models can overfit even with cross-validation

This is exactly why we evaluate models on completely separate test data - to catch overfitting that CV might miss.

### Real-World Recommendation

**Winner: LASSO Regression** for these reasons:

1.  **Lowest test RMSE** (1,961.35) - best actual performance

2.  **Consistent CV vs test performance** - reliable in production

3.  **Automatic feature selection** - identifies most important variables

4.  **Interpretable coefficients** - explainable to business stakeholders

5.  **22% error rate** relative to mean price - acceptable for business use

### The Simplicity Advantage

This analysis demonstrates that more complex doesn't always mean better. Flight pricing appears to follow fairly linear patterns that don't require the complexity of boosting algorithms. The linear relationships between factors like duration, airline, route, and timing are strong enough that simple models outperform complex ones.

### Production Deployment Insight

If you were to deploy this model for real flight price prediction: - LASSO would likely maintain its \~1,961 RMSE performance - XGBoost might perform much worse than expected due to overfitting - The 22% average error gives you realistic expectations for business planning

This comparison showcases why rigorous model evaluation with separate test sets is essential for reliable machine learning in business applications.
